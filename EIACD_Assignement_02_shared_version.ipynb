{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmfcardeira/EIACD_Assignement_02/blob/main/EIACD_Assignement_02_shared_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Introduction\n",
        "---\n",
        "\n",
        "This notebook was developed by the following students, under the course **CC1023 – Elementos de Inteligência Artificial e Ciência de Dados 2024/2025**, of the Faculty of Sciences of the University of Porto (FCUP):\n",
        "\n",
        "**Matilde Amorim – 202208540**\n",
        "\n",
        "**Rita Saraiva – 202207331**\n",
        "\n",
        "**Rodrigo Cardeira – 202206533**\n",
        "\n",
        "This notebook explores a dataset related to student performance in secondary education.\n",
        "\n",
        "The goal is to analyze the factors influencing student academic success, thus allowing to build an intervention system that may flag individual students requiring extra attention and support.\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "The dataset used in this analysis contains information about student demographics, social and school-related factors, and academic performance. We will use various data analysis and machine learning techniques to gain insights from this data.\n",
        "\n",
        "**Key Questions:**\n",
        "\n",
        "*   What are the key factors that influence student performance (passing/failing)?\n",
        "*   Can we build a model to predict student success based on their characteristics?\n",
        "*   What insights can we gain from this analysis to potentially improve student outcomes?\n",
        "\n",
        "**Analysis Steps:**\n",
        "\n",
        "1.  Data Exploration (EDA): Understanding the data structure, content, and identifying patterns.\n",
        "2.  Data Preprocessing: Data Cleaning, data transformation and feature engineering\n",
        "3.  Data Modeling (Supervised Learning): Choosing and training suitable machine learning models.\n",
        "4.  Perfomance Evaluation: Assessing and optimizing model performance.\n",
        "5.  Interpretation of Results: Drawing conclusions and suggesting potential interventions.\n"
      ],
      "metadata": {
        "id": "CzrYESeJ-rxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#0. Setup and Library Imports\n",
        "\n",
        "---\n",
        "Initial setup and configurations, as well as the importing of necessary libraries."
      ],
      "metadata": {
        "id": "WgYz60XB1rNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWmo4dE-o9rn",
        "outputId": "1529141a-728e-4b99-b169-58cc125f337d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup configured and libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure visualizations\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"Setup configured and libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 1. Data Exploration (EDA)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In this phase, we load the data and perform an initial analysis to understand its structure, content, and identify potential issues or characteristics relevant for preprocessing and modeling."
      ],
      "metadata": {
        "id": "R1Fri9PI1w04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Load the Dataset\n",
        "\n",
        "First, we load the `student-data.csv` file into a pandas DataFrame and display the first few rows to get an initial look at the data structure and content."
      ],
      "metadata": {
        "id": "s5exylrWq9lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = 'student-data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "# Display the last 5 rows\n",
        "print(\"\\nLast 5 rows of the dataset:\")\n",
        "display(df.tail())"
      ],
      "metadata": {
        "id": "sSWa0Cvbq_Tk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "20318b57-1448-4c4b-ace5-3977733654ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/student-data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c61050dc90af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/student-data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/student-data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Initial Data Inspection\n",
        "\n",
        "Let's get some basic information about the dataset:\n",
        "*   Number of records (students) and features (columns).\n",
        "*   Column names.\n",
        "*   Data types of each column.\n",
        "*   Check for missing values.\n",
        "*   Check for duplicate rows."
      ],
      "metadata": {
        "id": "o00QlhZbrDpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the dataset (rows, columns)\n",
        "print(f\"Dataset Shape: {df.shape[0]} records and {df.shape[1]} features.\\n\")\n",
        "\n",
        "# Get column names\n",
        "print(\"Column Names:\")\n",
        "print(list(df.columns))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get data types and non-null counts\n",
        "print(\"\\nData Types and Non-Null Counts:\")\n",
        "df.info()\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Check for missing values in each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "if missing_values.sum() == 0:\n",
        "    print(\"No missing values found.\")\n",
        "else:\n",
        "    print(f\"Total missing values: {missing_values.sum()}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicate_rows = df.duplicated().sum()\n",
        "print(f\"\\nNumber of Duplicate Rows: {duplicate_rows}\")\n",
        "if duplicate_rows > 0:\n",
        "    print(\"Consider removing duplicate rows in the preprocessing step.\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "DjS6y-cGrHA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from Initial Inspection:**\n",
        "\n",
        "*   The dataset contains 395 student records and 31 features.\n",
        "*   The features cover a range of demographic, social, school-related, and behavioral attributes.\n",
        "*   The target variable appears to be `passed` (yes/no).\n",
        "*   Most features are categorical (`object` type) or discrete numerical (`int64`). Many binary categorical features (like `schoolsup`, `famsup`, `paid`, etc.) are stored as strings.\n",
        "*   **Crucially, there are no missing values** in the dataset according to `df.info()` and `df.isnull().sum()`.\n",
        "*   **No duplicate rows** were found."
      ],
      "metadata": {
        "id": "_HAShi2ZrLuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Descriptive Statistics"
      ],
      "metadata": {
        "id": "MV7DmCPPwW4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    print(\"\\nDescriptive statistics for numerical features:\")\n",
        "    print(df.describe(include=[np.number])) # For numerical columns\n",
        "\n",
        "    print(\"\\nDescriptive statistics for categorical (object) features:\")\n",
        "    print(df.describe(include=['object'])) # For object columns"
      ],
      "metadata": {
        "id": "Sfqe0uduwYEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from Descriptive Statistics:**\n",
        "\n",
        "**Numerical Features:**\n",
        "\n",
        "age: Ranges from 15 to 22, with a mean of about 16.7. Most students are between 16 and 18. The max age of 22 might be an outlier or represent students repeating years.\n",
        "\n",
        "Medu (Mother's education) & Fedu (Father's education): Range from 0 (none) to 4 (higher education).\n",
        "\n",
        "traveltime: 1 (<15 min) to 4 (>1 hour). Most students live relatively close (1 or 2).\n",
        "\n",
        "studytime: 1 (<2 hours) to 4 (>10 hours). Most study 2-5 hours (category 2).\n",
        "\n",
        "failures: Number of past class failures (0 to 3, n if 1<=n<3, else 4 - but data shows max 3). Most students have 0 failures.\n",
        "\n",
        "famrel (Quality of family relationships): 1 (very bad) to 5 (excellent). Generally good.\n",
        "\n",
        "freetime, goout, Dalc (Workday alcohol), Walc (Weekend alcohol): Graded 1 (very low) to 5 (very high).\n",
        "\n",
        "health: 1 (very bad) to 5 (very good).\n",
        "\n",
        "absences: Ranges from 0 to 75. The mean is around 5.7, but the standard deviation is high (8), and the max of 75 suggests potential outliers or data entry issues. We should investigate this further.\n",
        "\n",
        "**Categorical Features:**\n",
        "\n",
        "school: Two schools, 'GP' (Gabriel Pereira) is more frequent (349 students) than 'MS' (Mousinho da Silveira, 46 students). This is an imbalance.\n",
        "\n",
        "sex: More 'F' (female, 208) than 'M' (male, 187). Fairly balanced.\n",
        "\n",
        "address: Mostly 'U' (urban, 307) vs 'R' (rural, 88).\n",
        "\n",
        "famsize: Mostly 'GT3' (greater than 3, 281) vs 'LE3' (less or equal to 3, 114).\n",
        "\n",
        "Pstatus (Parent's cohabitation status): Mostly 'T' (together, 354) vs 'A' (apart, 41).\n",
        "\n",
        "Mjob, Fjob: 'other' is the most common category. 'teacher' and 'services' are also prominent. 'at_home' and 'health' are less common.\n",
        "\n",
        "reason: 'course' preference is the most common reason for choosing the school.\n",
        "\n",
        "guardian: 'mother' is the most common guardian.\n",
        "\n",
        "Binary categorical features (yes/no): schoolsup, famsup, paid, activities, nursery, higher, internet, romantic.\n",
        "\n",
        "higher (wants to take higher education): Most students ('yes', 375) want to.\n",
        "\n",
        "internet (internet access at home): Most students ('yes', 329) have it.\n",
        "\n",
        "passed (Target Variable): More students 'yes' (passed, 265) than 'no' (failed, 130). This shows a moderate class imbalance."
      ],
      "metadata": {
        "id": "ZeawOqPYwaNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Feature Analysis\n",
        "\n",
        "Now, let's analyze the features in more detail, separating them into numerical and categorical types.\n",
        "\n",
        "#### 1.3.1 Numerical Features\n",
        "\n",
        "We'll look at the statistical summary and distributions of numerical features."
      ],
      "metadata": {
        "id": "Za--dOFbrRXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical features (based on df.info())\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "print(\"Numerical Features:\")\n",
        "print(numerical_cols)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get descriptive statistics for numerical features\n",
        "print(\"\\nDescriptive Statistics for Numerical Features:\")\n",
        "display(df[numerical_cols].describe())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Visualize distributions of numerical features\n",
        "print(\"\\nDistributions of Numerical Features:\")\n",
        "df[numerical_cols].hist(figsize=(15, 12), bins=15, layout=(-1, 4))\n",
        "plt.suptitle('Histograms of Numerical Features', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize potential outliers using boxplots\n",
        "print(\"\\nBoxplots of Numerical Features (to identify potential outliers):\")\n",
        "plt.figure(figsize=(18, 10))\n",
        "sns.boxplot(data=df[numerical_cols], orient='h')\n",
        "plt.title('Boxplots of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy-tDCcmrT3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations for Numerical Features:**\n",
        "\n",
        "*   **Age:** Ranges from 15 to 22. Most students are between 15 and 18. There are fewer older students (19+), which might be considered outliers or a specific subgroup.\n",
        "*   **Medu, Fedu (Mother's/Father's Education):** Ordinal scale (0-4). Most parents have some level of education (values > 0). Value 0 might indicate no education or missing information (though no NaNs were reported).\n",
        "*   **traveltime, studytime:** Ordinal scales (1-4). Most students have relatively short travel times and moderate study times (1-2 hours or 2-5 hours).\n",
        "*   **failures:** Number of past class failures (0-3). Most students have 0 failures. Value 3 likely represents '3 or more' failures. This feature is highly skewed.\n",
        "*   **famrel, freetime, goout, Dalc, Walc, health:** Ordinal scales (1-5). Distributions vary. `Dalc` (weekday alcohol) and `Walc` (weekend alcohol) are skewed towards lower consumption.\n",
        "*   **absences:** Number of school absences. Highly skewed to the right, ranging from 0 to 75. The value 75 seems like a significant outlier. Many students have 0 absences.\n",
        "*   **Outliers:** `absences` clearly shows potential outliers. `age` has a few values (20, 21, 22) that are less common. `failures` has a group at 3, which might represent an aggregation."
      ],
      "metadata": {
        "id": "a0G2IqgUrYtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.2 Categorical Features\n",
        "\n",
        "Let's examine the unique values and distributions for categorical features (including binary 'yes'/'no' features)."
      ],
      "metadata": {
        "id": "RpMEFognrhAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select categorical features (object type)\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "print(\"Categorical Features:\")\n",
        "print(categorical_cols)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Analyze value counts for each categorical feature\n",
        "print(\"\\nValue Counts for Categorical Features:\")\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n--- {col} ---\")\n",
        "    print(df[col].value_counts())\n",
        "    # Optional: Add percentage\n",
        "    # print(df[col].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualize distributions of categorical features\n",
        "print(\"\\nDistributions of Categorical Features:\")\n",
        "num_plots = len(categorical_cols)\n",
        "num_cols_grid = 4\n",
        "num_rows_grid = (num_plots + num_cols_grid - 1) // num_cols_grid # Calculate rows needed\n",
        "\n",
        "fig, axes = plt.subplots(num_rows_grid, num_cols_grid, figsize=(16, num_rows_grid * 4))\n",
        "axes = axes.flatten() # Flatten to easily iterate\n",
        "\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    sns.countplot(data=df, y=col, ax=axes[i], order=df[col].value_counts().index, hue=col, palette='viridis', legend=False)\n",
        "    axes[i].set_title(f'Distribution of {col}')\n",
        "    axes[i].set_xlabel('Count')\n",
        "    axes[i].set_ylabel('') # Remove y-label for clarity with y-ticks\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9lr0Dudtrj5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations for Categorical Features:**\n",
        "\n",
        "*   **school:** Two schools involved: 'GP' (Gabriel Pereira) and 'MS' (Mousinho da Silveira). 'GP' has significantly more students in this dataset.\n",
        "*   **sex:** Slightly more female ('F') students than male ('M').\n",
        "*   **address:** Most students live in Urban ('U') areas compared to Rural ('R').\n",
        "*   **famsize:** Family size is predominantly 'GT3' (Greater than 3) compared to 'LE3' (Less than or equal to 3).\n",
        "*   **Pstatus:** Parents' cohabitation status is mostly 'T' (Together) compared to 'A' (Apart).\n",
        "*   **Mjob, Fjob:** Diverse range of jobs. 'other' is the most common category for both parents. 'teacher' and 'services' are also frequent. 'at_home' is more common for mothers.\n",
        "*   **reason:** Most common reasons for choosing the school are 'course' preference, followed by 'home' proximity and 'reputation'.\n",
        "*   **guardian:** Most students have 'mother' as their guardian, followed by 'father', then 'other'.\n",
        "*   **schoolsup, famsup, paid, activities, nursery, higher, internet, romantic:** These are binary features ('yes'/'no').\n",
        "    *   `schoolsup`: Most students do *not* have extra educational support.\n",
        "    *   `famsup`: About half the students have family educational support.\n",
        "    *   `paid`: Most students do *not* take extra paid classes.\n",
        "    *   `activities`: About half the students participate in extra-curricular activities.\n",
        "    *   `nursery`: Most students attended nursery school.\n",
        "    *   `higher`: The vast majority of students want to pursue higher education. **This is highly imbalanced towards 'yes'.**\n",
        "    *   `internet`: Most students have internet access at home.\n",
        "    *   `romantic`: Most students are *not* in a romantic relationship.\n",
        "*   **passed (Target Variable):** More students passed ('yes') than failed ('no'). There is an imbalance, but it might not be severe enough to require complex handling initially. We need to quantify this."
      ],
      "metadata": {
        "id": "bbl6JdWCrkp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantify the target variable distribution\n",
        "print(\"\\nTarget Variable Distribution ('passed'):\")\n",
        "passed_counts = df['passed'].value_counts()\n",
        "passed_perc = df['passed'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(passed_counts)\n",
        "print(\"\\nPercentages:\")\n",
        "print(passed_perc)\n",
        "\n",
        "# Visualize target variable distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "# Updated countplot call:\n",
        "sns.countplot(data=df, x='passed', hue='passed', palette='Paired', legend=False)\n",
        "plt.title('Distribution of Target Variable (passed)')\n",
        "plt.xlabel('Passed Final Exam')\n",
        "plt.ylabel('Number of Students')\n",
        "\n",
        "# Add percentages to the plot (using iloc for position-based access)\n",
        "total = len(df)\n",
        "for i, count in enumerate(passed_counts):\n",
        "    plt.text(i, count + 5, f'{passed_perc.iloc[i]:.1f}% ({count})', ha='center')\n",
        "plt.ylim(0, max(passed_counts) * 1.15)  # Adjust y-limit for text\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4azC_DL7rm5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Variable Observation:**\n",
        "\n",
        "*   Approximately 67.1% of students passed ('yes'), while 32.9% failed ('no'). This confirms an imbalance where the 'passed' class is roughly twice the size of the 'failed' class."
      ],
      "metadata": {
        "id": "0QSgtC8eroeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Preliminary Relationship Analysis (Feature vs. Target)\n",
        "\n",
        "Let's briefly explore how some features relate to the target variable `passed`.\n",
        "\n",
        "#### 1.4.1 Numerical Features vs. `passed`"
      ],
      "metadata": {
        "id": "vGGDfEQOrsYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numerical Features vs. Passed Status:\")\n",
        "\n",
        "# Select a few key numerical features to compare against 'passed'\n",
        "num_cols_to_compare = ['age', 'failures', 'absences', 'studytime', 'goout', 'Dalc', 'Walc']\n",
        "\n",
        "fig, axes = plt.subplots(len(num_cols_to_compare), 1, figsize=(8, len(num_cols_to_compare) * 4))\n",
        "if len(num_cols_to_compare) == 1: # Handle case of single plot\n",
        "    axes = [axes]\n",
        "\n",
        "for i, col in enumerate(num_cols_to_compare):\n",
        "    # Assign 'passed' to 'hue' and set legend=False\n",
        "    sns.boxplot(data=df, x='passed', y=col, ax=axes[i], hue='passed', palette='Paired', legend=False)\n",
        "    axes[i].set_title(f'{col} vs. Passed Status')\n",
        "    axes[i].set_xlabel('Passed Final Exam')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r2uoJzuDrwlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations (Numerical vs. Passed):**\n",
        "\n",
        "*   **failures:** Students who failed ('no') tend to have significantly more past failures than those who passed ('yes'). This looks like a strong predictor.\n",
        "*   **absences:** Students who failed seem to have slightly more absences on average, but the distributions have large overlaps and many outliers.\n",
        "*   **studytime:** Students who passed seem to report slightly higher study times, but the difference is not dramatic based on the boxplot medians/quartiles.\n",
        "*   **goout:** Students who failed tend to report going out more frequently.\n",
        "*   **Dalc, Walc:** Alcohol consumption appears slightly higher for students who failed, particularly weekend consumption (`Walc`), but median values are often the same.\n",
        "*   **age:** Older students seem slightly more likely to fail, but the distributions overlap considerably."
      ],
      "metadata": {
        "id": "H4vnw650rxtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2 Categorical Features vs. `passed`"
      ],
      "metadata": {
        "id": "QsPrIDeer0J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCategorical Features vs. Passed Status:\")\n",
        "\n",
        "# Select a few key categorical features to compare against 'passed'\n",
        "cat_cols_to_compare = ['sex', 'Medu', 'Fedu', 'schoolsup', 'higher', 'romantic', 'internet']\n",
        "\n",
        "# Use countplot with 'hue' for visualization\n",
        "fig, axes = plt.subplots(len(cat_cols_to_compare), 1, figsize=(10, len(cat_cols_to_compare) * 4.5))\n",
        "if len(cat_cols_to_compare) == 1: # Handle case of single plot\n",
        "    axes = [axes]\n",
        "\n",
        "for i, col in enumerate(cat_cols_to_compare):\n",
        "    order = sorted(df[col].unique()) if col in ['Medu', 'Fedu'] else None # Order education levels\n",
        "    sns.countplot(data=df, x=col, hue='passed', ax=axes[i], palette='Paired', order=order)\n",
        "    axes[i].set_title(f'{col} vs. Passed Status')\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].legend(title='Passed')\n",
        "    # Add percentages within each category (optional, can make plots busy)\n",
        "    # for container in axes[i].containers:\n",
        "    #    axes[i].bar_label(container, fmt='%.0f')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Crosstab for 'higher' might be more revealing due to its imbalance\n",
        "print(\"\\nCrosstab: higher vs passed\")\n",
        "display(pd.crosstab(df['higher'], df['passed'], normalize='index') * 100) # Show percentage within 'higher' category"
      ],
      "metadata": {
        "id": "4XEA82e-r2pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations (Categorical vs. Passed):**\n",
        "\n",
        "*   **sex:** Pass rates seem roughly similar between males and females, perhaps slightly higher for females.\n",
        "*   **Medu, Fedu:** Higher parental education levels appear correlated with a higher pass rate. Students whose parents have higher education (e.g., levels 3 and 4) are more likely to pass.\n",
        "*   **schoolsup:** Students *with* school support ('yes') have a noticeably lower pass rate than those without. This is counter-intuitive and needs investigation. Perhaps students receiving support are already those struggling academically?\n",
        "*   **higher:** Almost all students who want to go on to higher education ('yes') passed. Conversely, a large proportion of students *not* wanting to go to higher education ('no') failed. This seems like a very strong indicator.\n",
        "*   **romantic:** Students in a romantic relationship ('yes') appear to have a lower pass rate.\n",
        "*   **internet:** Having internet access seems slightly associated with a higher pass rate."
      ],
      "metadata": {
        "id": "PSfsUBK2r5FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Summary of Data Exploration Findings & Potential Issues\n",
        "\n",
        "1.  **Dataset Size:** 395 records, 31 features. Manageable size.\n",
        "2.  **Target Variable:** `passed` (Categorical: 'yes'/'no').\n",
        "3.  **Class Imbalance:** The target variable is imbalanced (approx. 67% 'yes', 33% 'no'). This should be considered during modeling and evaluation (e.g., using appropriate metrics like F1-score, Precision, Recall, AUC, and potentially resampling techniques).\n",
        "4.  **Feature Types:** Mix of numerical (mostly discrete/ordinal) and categorical (many binary 'yes'/'no'). Categorical features will need encoding (e.g., One-Hot, Ordinal) for most ML models. Binary 'yes'/'no' features can be easily mapped to 1/0.\n",
        "5.  **Missing Values:** None found.\n",
        "6.  **Duplicates:** None found.\n",
        "7.  **Outliers:** `absences` has significant potential outliers. `age` has a few older students. Decisions on handling these (e.g., clipping, removal, transformation) should be made during preprocessing.\n",
        "8.  **Potential Predictors:** Features like `failures`, `higher`, `Medu`, `Fedu`, `schoolsup`, `goout`, `studytime`, and `romantic` showed noticeable associations with the `passed` status in the preliminary analysis and warrant further investigation.\n",
        "9.  **Highly Skewed Features:** `failures` and `absences` are highly skewed. Transformations (e.g., log transform for `absences` if appropriate) might be considered.\n",
        "\n",
        "10. **Underrepresented Categories:**\n",
        "    `school`: 'MS' has significantly fewer students than 'GP'.\n",
        "     Some job categories (`Mjob`, `Fjob`) and `guardian`='other' have low frequencies.\n",
        "     This could impact model performance or generalization, especially if these rare categories are important predictors.\n",
        "11. **Potential Irrelevant Features (Hypothesis - to be confirmed with further analysis/modeling):**\n",
        "       It's hard to definitively say at this stage. Domain knowledge suggests most of these features could be relevant to student performance. Feature selection techniques will be important later.\n",
        "       For example, `nursery` (attended nursery school) might have less impact on secondary school performance compared to more recent factors.\n",
        "12. **Data Scale:**\n",
        "      Numerical features are on different scales (e.g., `age` vs `absences`).\n",
        "       Scaling/Normalization will be necessary for algorithms sensitive to feature magnitudes (e.g., KNN, SVM, Neural Networks)\n",
        "10. **Feature Redundancy:** Although not explicitly checked with a correlation matrix for numerical features (omitted here for brevity, but recommended), potential redundancy might exist (e.g., `Medu` and `Fedu`, `Dalc` and `Walc`). `Dalc` and `Walc` could potentially be combined into a total alcohol consumption feature.\n",
        "11. **Counter-intuitive Relationships:** The relationship between `schoolsup` and `passed` (students *with* support having lower pass rates) is unexpected and suggests that this feature might be capturing students who are already at risk.\n",
        "\n",
        "**Next Steps (Data Cleaning and Preprocessing):**\n",
        "\n",
        "*   Encode categorical features (binary and multi-class).\n",
        "*   Address outliers (especially in `absences`).\n",
        "*   Consider feature scaling for numerical features if using distance-based algorithms (like KNN) or models sensitive to scale (like some Neural Networks, SVMs).\n",
        "*   Handle class imbalance if initial model performance is poor for the minority class ('no').\n",
        "*   Potentially perform feature engineering (e.g., combining alcohol consumption) or feature selection.\n",
        "*   Further investigate correlations between features."
      ],
      "metadata": {
        "id": "-m5Bb6eHr-jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\n",
        "\\n",
        "---\\n",
        "\\n",
        "\\n",
        "#2. Data Cleaning and Preprocessing\\n",
        "\\n",
        "\\n",
        "---\\n",
        "\\n",
        "\\n",
        "\\n",
        "Based on the EDA, we will now clean and prepare the data for machine learning modeling. We will work on a copy."
      ],
      "metadata": {
        "id": "QbaZF96L3paU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create a Copy"
      ],
      "metadata": {
        "id": "-xTFyTXw4COZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Work on a copy to preserve the original data 'df'\\n",
        "df_processed = df.copy()\\n",
        "\\n",
        "print(\\"Working on a copy of the original DataFrame.\\")\\n",
        "print(f\\"Initial shape for processing: {df_processed.shape}\\")"
      ],
      "metadata": {
        "id": "tJWAEf1g4EGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Feature Cleaning and Preprocessing\\n",
        "\\n",
        "In this section, we will clean and transform the features identified during EDA to make them suitable for machine learning models. This includes encoding categorical variables, handling outliers and skewness in numerical variables, and potentially engineering new features."
      ],
      "metadata": {
        "id": "6mdycNy2a4Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1 Handling Categorical Features\\n",
        "\\n",
        "Most machine learning algorithms require numerical input. Therefore, we need to convert categorical features into a numerical format. We'll handle binary features by mapping them to 0/1 and multi-category nominal features using One-Hot Encoding."
      ],
      "metadata": {
        "id": "4KzJW2xJIwbv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_binary_intro_cat_encode"
      },
      "source": [
        "##### 2.2.1.1 Binary Categorical Features\\n",
        "\\n",
        "Binary categorical features are those that have exactly two unique values (e.g., 'yes'/'no', 'M'/'F'). We will map these to numerical representations (0 and 1) for easier processing by machine learning algorithms.\\n",
        "\\n",
        "*   For features with 'yes'/'no' values, 'yes' will be mapped to 1 and 'no' to 0. This includes the target variable `passed`.\\n",
        "*   For other binary features like `sex`, we will define a clear mapping (e.g., 'F' to 0, 'M' to 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_binary_impl_cat_encode"
      },
      "outputs": [],
      "source": [
        "# Identify binary categorical features\\n",
        "binary_yes_no_cols = []\\n",
        "other_binary_cols = [] \\n",
        "# Start with all object columns, and remove as they are processed\\n",
        "object_cols_to_process = df_processed.select_dtypes(include='object').columns.tolist()\\n",
        "\\n",
        "for col in object_cols_to_process[:]: # Iterate on a copy for safe removal\\n",
        "    unique_values = df_processed[col].unique()\\n",
        "    if len(unique_values) == 2:\\n",
        "        unique_values_list = list(unique_values) # Make it a list for easier checking\\n",
        "        if 'yes' in unique_values_list and 'no' in unique_values_list:\\n",
        "            binary_yes_no_cols.append(col)\\n",
        "            object_cols_to_process.remove(col)\\n",
        "        # Special handling for 'sex' or other known binary columns not 'yes'/'no'\\n",
        "        elif col == 'sex': \\n",
        "            other_binary_cols.append(col)\\n",
        "            object_cols_to_process.remove(col)\\n",
        "\\n",
        "print(f\\"Binary 'yes'/'no' columns (target 'passed' will be handled if in this list or separately): {binary_yes_no_cols}\\")\\n",
        "print(f\\"Other binary columns (e.g., 'sex'): {other_binary_cols}\\")\\n",
        "\\n",
        "# Map 'yes'/'no' to 1/0\\n",
        "for col in binary_yes_no_cols:\\n",
        "    df_processed[col] = df_processed[col].map({'yes': 1, 'no': 0})\\n",
        "    print(f\\"Mapped '{col}' to 1/0. Unique values after mapping: {df_processed[col].unique()}\\")\\n",
        "\\n",
        "# Explicitly map 'passed' if it's still an object and wasn't caught (should be)\\n",
        "if 'passed' in df_processed.columns and df_processed['passed'].dtype == 'object':\\n",
        "    df_processed['passed'] = df_processed['passed'].map({'yes': 1, 'no': 0})\\n",
        "    print(f\\"Mapped target 'passed' (explicitly, if needed) to 1/0. Unique values: {df_processed['passed'].unique()}\\")\\n",
        "    if 'passed' in object_cols_to_process: object_cols_to_process.remove('passed') # remove if it was still there\\n",
        "    if 'passed' not in binary_yes_no_cols: binary_yes_no_cols.append('passed') # ensure it's tracked as processed binary\\n",
        "\\n",
        "# Map 'sex' feature: 'F' to 0, 'M' to 1\\n",
        "if 'sex' in other_binary_cols:\\n",
        "    df_processed['sex'] = df_processed['sex'].map({'F': 0, 'M': 1})\\n",
        "    print(f\\"Mapped 'sex' to 0/1. Unique values: {df_processed['sex'].unique()}\\")\\n",
        "elif 'sex' in df_processed.columns and df_processed['sex'].dtype == 'object': # Fallback if 'sex' wasn't added to other_binary_cols\\n",
        "    df_processed['sex'] = df_processed['sex'].map({'F': 0, 'M': 1})\\n",
        "    print(f\\"Mapped 'sex' (fallback) to 0/1. Unique values: {df_processed['sex'].unique()}\\")\\n",
        "    if 'sex' in object_cols_to_process: object_cols_to_process.remove('sex')\\n",
        "\\n",
        "print(\\"\\\\nDataFrame head after binary mapping:\\")\\n",
        "display(df_processed.head())\\n",
        "print(\\"\\\\nDataFrame info after binary mapping:\\")\\n",
        "df_processed.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_onehot_intro_cat_encode"
      },
      "source": [
        "##### 2.2.1.2 Nominal Categorical Features (More than 2 Categories)\\n",
        "\\n",
        "Nominal categorical features with more than two unique values (e.g., 'Mjob', 'Fjob', 'reason', 'guardian', 'school') cannot be directly mapped to ordinal numbers as there's no inherent order. For these, we use One-Hot Encoding. This technique creates new binary (0 or 1) columns for each unique category in the original feature.\\n",
        "\\n",
        "We will use `pandas.get_dummies()` for this purpose. `drop_first=True` will be used to create k-1 dummy variables for k categories, which helps in avoiding multicollinearity when building some types of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_onehot_impl_cat_encode"
      },
      "outputs": [],
      "source": [
        "# Identify remaining nominal categorical features with more than two categories\\n",
        "# These are columns of 'object' type that were not converted in binary mapping\\n",
        "nominal_cols_to_encode = df_processed.select_dtypes(include='object').columns.tolist()\\n",
        "\\n",
        "print(f\\"Nominal columns to be one-hot encoded: {nominal_cols_to_encode}\\")\\n",
        "\\n",
        "# Apply One-Hot Encoding\\n",
        "if nominal_cols_to_encode:\\n",
        "    df_processed = pd.get_dummies(df_processed, columns=nominal_cols_to_encode, drop_first=True, dtype=int)\\n",
        "    print(\\"\\\\nApplied One-Hot Encoding.\\")\\n",
        "else:\\n",
        "    print(\\"\\\\nNo nominal columns found requiring One-Hot Encoding (all object types handled or already numeric).\\")\\n",
        "\\n",
        "# Display head and info to see the changes\\n",
        "print(\\"\\\\nDataFrame head after One-Hot Encoding:\\")\\n",
        "display(df_processed.head())\\n",
        "\\n",
        "print(\\"\\\\nDataFrame info after One-Hot Encoding:\\")\\n",
        "df_processed.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_numerical_handling_title_2"
      },
      "source": [
        "#### 2.2.2 Handling Numerical Features (Outliers and Skewness)\\n",
        "\\n",
        "Numerical features can sometimes contain outliers or be heavily skewed, which can impact the performance of certain machine learning models. We will investigate and address these issues, primarily focusing on the `absences` column as identified in the EDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_visualize_absences_before_2"
      },
      "outputs": [],
      "source": [
        "# Define the original list of numerical columns from EDA for later describe()\\n",
        "original_numerical_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\\n",
        "\\n",
        "print(\\"Distribution of 'absences' BEFORE outlier handling:\\")\\n",
        "plt.figure(figsize=(12, 5))\\n",
        "plt.subplot(1, 2, 1)\\n",
        "sns.histplot(df_processed['absences'], kde=True, bins=30) # Increased bins for more detail\\n",
        "plt.title('Histogram of Absences (Before)')\\n",
        "\\n",
        "plt.subplot(1, 2, 2)\\n",
        "sns.boxplot(y=df_processed['absences'])\\n",
        "plt.title('Boxplot of Absences (Before)')\\n",
        "plt.tight_layout()\\n",
        "plt.show()\\n",
        "\\n",
        "print(df_processed['absences'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_absences_strategy_2"
      },
      "source": [
        "The `absences` column shows a significant number of outliers on the higher end (max value of 75, while 75% of students have 8 or fewer absences). A log transform could be an option, but given the presence of many zeros (which `np.log1p` handles) and the discrete nature of counts, **clipping at a high percentile (e.g., 95th)** is a more direct way to reduce the influence of extreme values without drastically changing the distribution's nature for the majority of the data. This approach retains the original scale for most students while mitigating the impact of very high, potentially erroneous or overly influential, values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_handle_absences_outliers_2"
      },
      "outputs": [],
      "source": [
        "absences_q95 = df_processed['absences'].quantile(0.95)\\n",
        "print(f\\"95th percentile for 'absences': {absences_q95}\\")\\n",
        "\\n",
        "df_processed['absences'] = df_processed['absences'].clip(upper=absences_q95)\\n",
        "print(\\"\\\\n'absences' column clipped at the 95th percentile.\\")\\n",
        "\\n",
        "print(\\"\\\\nDistribution of 'absences' AFTER outlier handling:\\")\\n",
        "plt.figure(figsize=(12, 5))\\n",
        "plt.subplot(1, 2, 1)\\n",
        "sns.histplot(df_processed['absences'], kde=True, bins=15)\\n",
        "plt.title('Histogram of Absences (After)')\\n",
        "\\n",
        "plt.subplot(1, 2, 2)\\n",
        "sns.boxplot(y=df_processed['absences'])\\n",
        "plt.title('Boxplot of Absences (After)')\\n",
        "plt.tight_layout()\\n",
        "plt.show()\\n",
        "\\n",
        "print(\\"\\\\nDescriptive statistics for 'absences' AFTER clipping:\\")\\n",
        "print(df_processed['absences'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_other_numerical_features_2"
      },
      "source": [
        "##### Other Numerical Features\\n",
        "\\n",
        "Other numerical features identified in the EDA include `age`, `Medu`, `Fedu`, `traveltime`, `studytime`, `failures`, `famrel`, `freetime`, `goout`, `Dalc`, `Walc`, and `health`.\\n",
        "\\n",
        "*   **`age`**: While there are a few older students (up to 22), these values are not necessarily errors and could represent a valid subgroup. We will retain these values as is for now.\\n",
        "*   **Ordinal Features & Other Counts** (`Medu`, `Fedu`, `traveltime`, `studytime`, `failures`, `famrel`, `freetime`, `goout`, `Dalc`, `Walc`, `health`): These features are mostly ordinal or represent counts on a limited scale (e.g., 1-5 or 0-4). Outlier handling for these is typically less critical unless there are clear data entry errors (none apparent). The skewness in features like `failures` is inherent to its meaning (most students don't fail many times). Transformations could obscure their interpretability or might not be necessary, especially for tree-based models.\\n",
        "\\n",
        "For now, we have focused the outlier treatment on `absences`. Other numerical features will be kept as they are. Scaling (e.g., Standardization or Normalization) might be considered in a subsequent step if required by the chosen machine learning model (e.g., SVM, KNN, or Neural Networks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_describe_numerical_after_2"
      },
      "outputs": [],
      "source": [
        "print(\\"Descriptive statistics for original numerical features AFTER processing 'absences':\\")\\n",
        "# Re-define original_numerical_cols list here in case this cell is run in isolation in the future.\\n",
        "original_numerical_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\\n",
        "display(df_processed[original_numerical_cols].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_feature_scaling_title_3"
      },
      "source": [
        "#### 2.2.3 Feature Scaling\\n",
        "\\n",
        "After handling categorical features and outliers, the next step for some numerical features is scaling. This ensures that features with larger value ranges do not dominate those with smaller ranges in algorithms that are sensitive to feature magnitudes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_feature_scaling_explanation_3"
      },
      "source": [
        "##### Why Scale Features?\\n",
        "\\n",
        "Many machine learning algorithms, especially those that calculate distances between data points (e.g., K-Nearest Neighbors, Support Vector Machines) or use gradient descent optimization (e.g., Linear Regression, Logistic Regression, Neural Networks), perform better or converge faster when numerical features are on a similar scale.\\n",
        "\\n",
        "If features are not scaled:\\n",
        "*   Features with larger values might disproportionately influence the model.\\n",
        "*   Gradient descent algorithms might take longer to converge.\\n",
        "\\n",
        "Tree-based algorithms (like Decision Trees, Random Forests, Gradient Boosting) are generally not sensitive to feature scaling. However, scaling is often still performed as good practice, especially if different types of models are being explored or if the scaled data aids in interpreting coefficients or feature importances in some contexts.\\n",
        "\\n",
        "##### StandardScaler\\n",
        "\\n",
        "We will use `StandardScaler` from scikit-learn. This method standardizes features by removing the mean and scaling to unit variance. The standard score of a sample `x` is calculated as:\\n",
        "\\n",
        "`z = (x - u) / s`\\n",
        "\\n",
        "where `u` is the mean of the training samples, and `s` is the standard deviation of the training samples.\\n",
        "This results in a distribution with a mean of 0 and a standard deviation of 1.\\n",
        "\\n",
        "Another common option is `MinMaxScaler`, which scales features to a specific range, typically [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_perform_scaling_3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\\n",
        "\\n",
        "# Numerical columns to scale (identified from EDA and previous steps)\\n",
        "# These are the original numerical columns; 'absences' has already been clipped.\\n",
        "numerical_cols_for_scaling = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\\n",
        "\\n",
        "# Verify these columns are present in df_processed and are numeric\\n",
        "print(\\"Data types of columns selected for scaling (before scaling):\\")\\n",
        "print(df_processed[numerical_cols_for_scaling].dtypes)\\n",
        "print(\\"\\\\nDescriptive statistics BEFORE scaling:\\")\\n",
        "display(df_processed[numerical_cols_for_scaling].describe())\\n",
        "\\n",
        "# Initialize the StandardScaler\\n",
        "scaler = StandardScaler()\\n",
        "\\n",
        "# Fit the scaler to the numerical columns and transform them\\n",
        "# Note: It's crucial to fit the scaler ONLY on the training data in a real train/test split scenario.\\n",
        "# Here, we are preprocessing the entire dataset before splitting, which is common for EDA and initial setup.\\n",
        "df_processed[numerical_cols_for_scaling] = scaler.fit_transform(df_processed[numerical_cols_for_scaling])\\n",
        "\\n",
        "print(\\"\\\\nNumerical features scaled using StandardScaler.\\")\\n",
        "\\n",
        "print(\\"\\\\nDataFrame head for scaled numerical features:\\")\\n",
        "display(df_processed[numerical_cols_for_scaling].head())\\n",
        "\\n",
        "print(\\"\\\\nDescriptive statistics AFTER scaling (mean should be ~0, std ~1):\\")\\n",
        "display(df_processed[numerical_cols_for_scaling].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_class_imbalance_title_4"
      },
      "source": [
        "#### 2.2.4 Handling Class Imbalance\\n",
        "\\n",
        "Class imbalance occurs when the distribution of classes in the target variable is unequal. This is a common issue in many datasets and can significantly affect the performance and evaluation of machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_show_target_distribution_4"
      },
      "outputs": [],
      "source": [
        "print(\\"Current distribution of the target variable 'passed' in df_processed:\\")\\n",
        "# The 'passed' column should have been mapped to 0 (failed) and 1 (passed) earlier.\\n",
        "passed_distribution = df_processed['passed'].value_counts(normalize=True) * 100\\n",
        "print(passed_distribution)\\n",
        "print(\\"\\\\n0 typically represents 'no' (failed) and 1 represents 'yes' (passed).\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_class_imbalance_discussion_4"
      },
      "source": [
        "As observed in the EDA and confirmed by the output above, our target variable `passed` exhibits a moderate imbalance. The majority class (students who passed, typically coded as 1) is more prevalent than the minority class (students who failed, typically coded as 0).\\n",
        "\\n",
        "##### Potential Problems with Class Imbalance:\\n",
        "*   **Model Bias:** Models trained on imbalanced data may become biased towards the majority class. They might achieve high accuracy by simply predicting the majority class most of the time, but perform poorly on the minority class, which is often the class of greater interest (e.g., predicting failures).\\n",
        "*   **Misleading Evaluation Metrics:** Standard accuracy can be a misleading metric. A model could have 90% accuracy but still miss all instances of a minority class that constitutes 10% of the data.\\n",
        "\\n",
        "##### Common Techniques to Handle Class Imbalance:\\n",
        "There are several strategies to address class imbalance:\\n",
        "1.  **Resampling Techniques:**\\n",
        "    *   **Oversampling the Minority Class:** Techniques like SMOTE (Synthetic Minority Over-sampling Technique) create synthetic samples of the minority class.\\n",
        "    *   **Undersampling the Majority Class:** Techniques like RandomUnderSampler reduce the number of samples from the majority class. This can be useful for very large datasets but may lead to loss of information.\\n",
        "    *   **Combination of Oversampling and Undersampling:** Some methods combine both approaches (e.g., SMOTEENN, SMOTETomek).\\n",
        "2.  **Using Appropriate Evaluation Metrics:** Metrics like Precision, Recall, F1-score, AUC-ROC (Area Under the Receiver Operating Characteristic Curve), and AUC-PR (Area Under the Precision-Recall Curve) are more informative than accuracy for imbalanced datasets.\\n",
        "3.  **Cost-Sensitive Learning (Class Weights):** Many algorithms (e.g., Logistic Regression, SVMs, Random Forests, XGBoost) allow assigning different weights to classes, penalizing misclassifications of the minority class more heavily during training.\\n",
        "4.  **Algorithmic Approaches:** Some algorithms are inherently better at handling imbalance (e.g., some ensemble methods), or can be adapted to do so.\\n",
        "\\n",
        "##### Important Note on Resampling:\\n",
        "**If resampling techniques (like SMOTE or undersampling) are used, it is crucial that they are applied *only* to the training data *after* splitting the dataset into training and testing sets.** Applying resampling before splitting can lead to data leakage, where information from the test set inadvertently influences the training process, resulting in overly optimistic performance estimates on the test set.\\n",
        "\\n",
        "##### Conclusion for Current Preprocessing:\\n",
        "For now, we will **not** apply any resampling techniques directly to the `df_processed` DataFrame. The dataset will be kept in its current (imbalanced) state through this general preprocessing phase. The decision on how to handle class imbalance (e.g., by choosing appropriate models, using class weights during model training, or resampling only the training set post-split) will be a key consideration during the **Data Modeling** phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_preprocessed_summary_title_5"
      },
      "source": [
        "#### 2.2.5 Summary of Preprocessed Data\n",
        "\n",
        "Before splitting the data for model training, let's display the information and first few rows of our fully `df_processed` DataFrame to see its final state after all encoding, outlier treatment, and scaling steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_preprocessed_summary_output_5"
      },
      "outputs": [],
      "source": [
        "print(\"Information about the preprocessed DataFrame (df_processed):\")\n",
        "df_processed.info()\n",
        "\n",
        "print(\"\\nFirst 5 rows of the preprocessed DataFrame (df_processed):\")\n",
        "display(df_processed.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_data_splitting_main_title_5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Data Splitting\n",
        "\n",
        "---\n",
        "\n",
        "With the data cleaned and preprocessed, the next critical step is to split it into training and testing sets. This allows us to train our machine learning models on one portion of the data and then evaluate their performance on a separate, unseen portion, which is essential for assessing how well our models generalize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_purpose_of_splitting_5"
      },
      "source": [
        "### 3.1 Purpose of Splitting Data\n",
        "\n",
        "Splitting the dataset is essential for a reliable assessment of a model's ability to generalize to new, unseen data. \n",
        "*   **Training Set:** Used by the machine learning algorithm to learn patterns and relationships from the data.\n",
        "*   **Testing Set:** Used to evaluate the performance of the trained model on data it has not encountered during training. This helps to estimate how the model will perform in real-world scenarios.\n",
        "\n",
        "Without a separate test set, we risk **overfitting**, where the model learns the training data too well (including its noise) and fails to generalize to new data. The test set provides an unbiased estimate of this generalization performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_perform_data_split_5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "if 'passed' not in df_processed.columns:\n",
        "    raise KeyError(\"'passed' column not found in df_processed. Ensure it's correctly named and present.\")\n",
        "X = df_processed.drop('passed', axis=1)\n",
        "y = df_processed['passed'] # 'passed' is already 0 or 1\n",
        "\n",
        "print(\"Features (X) and target (y) separated.\")\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2,       # Using 20% of the data for testing\n",
        "    random_state=42,     # Ensures reproducibility of the split\n",
        "    stratify=y           # Preserves class proportions in train and test sets due to imbalance\n",
        ")\n",
        "\n",
        "print(\"\\nData successfully split into training and testing sets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_traintestsplit_params_5"
      },
      "source": [
        "### 3.2 `train_test_split` Parameters Used:\n",
        "\n",
        "*   `X`, `y`: The features matrix and the target vector, respectively.\n",
        "*   `test_size=0.2`: This parameter specifies that 20% of the data will be allocated to the test set, while the remaining 80% will be used for training. This is a common and reasonable split ratio.\n",
        "*   `random_state=42`: Setting `random_state` to an integer (42 is a conventional choice) ensures that the data split is identical every time the code is executed. This is crucial for reproducibility of results and experiments.\n",
        "*   `stratify=y`: This is particularly important for datasets with class imbalance, like ours. By setting `stratify=y`, the function ensures that the proportion of each class in the target variable (`passed`) is approximately the same in both the training and testing sets as it is in the original dataset. This helps to create more representative splits and ensures that the model evaluation is fair and not skewed by different class distributions in train versus test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_verify_split_5"
      },
      "outputs": [],
      "source": [
        "print(\"Shapes of the resulting datasets:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nProportion of target variable in original dataset (y):\")\n",
        "print(y.value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\nProportion of target variable in training set (y_train):\")\n",
        "print(y_train.value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\nProportion of target variable in testing set (y_test):\")\n",
        "print(y_test.value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\nVerification of stratification: The proportions of each class should be similar across y, y_train, and y_test.\")"
      ]
    }
  ]
}
